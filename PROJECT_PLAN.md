# Reachy Mini for Home Assistant - Project Plan

## Project Overview

Integrate Home Assistant voice assistant functionality into Reachy Mini Wi-Fi robot, communicating with Home Assistant via ESPHome protocol.

## Local Reference Directories (DO NOT modify any files in reference directories)
1. [linux-voice-assistant](reference/linux-voice-assistant) - Linux-based Home Assistant voice assistant app for reference
2. [Reachy Mini SDK](reference/reachy_mini) - Reachy Mini SDK local directory for reference
3. [reachy_mini_conversation_app](reference/reachy_mini_conversation_app) - Reachy Mini conversation app for reference
4. [reachy-mini-desktop-app](reference/reachy-mini-desktop-app) - Reachy Mini desktop app for reference
5. [sendspin](reference/sendspin-cli/) - Sendspin client for reference

## Core Design Principles

1. **Zero Configuration** - Users only need to install the app, no manual configuration required
2. **Native Hardware** - Use robot's built-in microphone and speaker
3. **Home Assistant Centralized Management** - All configuration done on Home Assistant side
4. **Motion Feedback** - Provide head movement and antenna animation feedback during voice interaction
5. **Project Constraints** - Strictly follow [Reachy Mini SDK](reachy_mini) architecture design and constraints
6. **Code Quality** - Follow Python development standards with consistent code style, clear structure, complete comments, comprehensive documentation, high test coverage, high code quality, readability, maintainability, extensibility, and reusability
7. **Feature Priority** - Voice conversation with Home Assistant is highest priority; other features are auxiliary and must not affect voice conversation functionality or response speed
8. **No LED Functions** - LEDs are hidden inside the robot; all LED control is ignored
9. **Preserve Functionality** - Any code modifications should optimize while preserving completed features; do not remove features to solve problems. When issues occur, prioritize solving problems after referencing examples, not adding various log outputs

## Technical Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              Reachy Mini (ARM64)                            â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AUDIO INPUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ReSpeaker XVF3800 (16kHz)                                            â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ 4-Mic Array  â”‚ â†’ â”‚ XVF3800 DSP                                  â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â€¢ Echo Cancellation (AEC)                    â”‚  â”‚  â”‚
â”‚  â”‚                     â”‚ â€¢ Noise Suppression (NS)                     â”‚  â”‚  â”‚
â”‚  â”‚                     â”‚ â€¢ Auto Gain Control (AGC, max 30dB)          â”‚  â”‚  â”‚
â”‚  â”‚                     â”‚ â€¢ Direction of Arrival (DOA)                 â”‚  â”‚  â”‚
â”‚  â”‚                     â”‚ â€¢ Voice Activity Detection (VAD)             â”‚  â”‚  â”‚
â”‚  â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                      â”‚                                â”‚  â”‚
â”‚  â”‚                                      â–¼                                â”‚  â”‚
â”‚  â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚                     â”‚ Wake Word Detection (microWakeWord)          â”‚  â”‚  â”‚
â”‚  â”‚                     â”‚ â€¢ "Okay Nabu" / "Hey Jarvis"                 â”‚  â”‚  â”‚
â”‚  â”‚                     â”‚ â€¢ Stop word detection                        â”‚  â”‚  â”‚
â”‚  â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AUDIO OUTPUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ TTS Player               â”‚    â”‚ Music Player (Sendspin)          â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Voice assistant speech â”‚    â”‚ â€¢ Multi-room audio streaming     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Sound effects          â”‚    â”‚ â€¢ Auto-discovery via mDNS        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Priority over music    â”‚    â”‚ â€¢ Auto-pause during conversation â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                 â”‚                              â”‚                      â”‚  â”‚
â”‚  â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚  â”‚
â”‚  â”‚                                â–¼                                      â”‚  â”‚
â”‚  â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚                 â”‚ ReSpeaker Speaker (16kHz)                        â”‚  â”‚  â”‚
â”‚  â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ VISION & TRACKING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ Camera (VPU accelerated) â”‚ â†’  â”‚ YOLO Face Detection              â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ MJPEG stream server    â”‚    â”‚ â€¢ AdamCodd/YOLOv11n-face         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ ESPHome Camera entity  â”‚    â”‚ â€¢ Adaptive frame rate:           â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   - 15fps: conversation/face     â”‚ â”‚  â”‚
â”‚  â”‚                                  â”‚   - 2fps: idle (power saving)    â”‚ â”‚  â”‚
â”‚  â”‚                                  â”‚ â€¢ look_at_image() pose calc      â”‚ â”‚  â”‚
â”‚  â”‚                                  â”‚ â€¢ Smooth return after face lost  â”‚ â”‚  â”‚
â”‚  â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MOTION CONTROL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MovementManager (100Hz Control Loop)                                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ Motion Layers (Priority: Move > Action > SpeechSway > Breath)  â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ â”‚ Move Queue â”‚ â”‚ Actions    â”‚ â”‚ SpeechSway â”‚ â”‚ Breathing    â”‚  â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ â”‚ (Emotions) â”‚ â”‚ (Nod/Shake)â”‚ â”‚ (Voice VAD)â”‚ â”‚ (Idle anim)  â”‚  â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ Face Tracking Offsets (Secondary Pose Overlay)                 â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Pitch offset: +9Â° (down compensation)                        â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Yaw offset: -7Â° (right compensation)                         â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚   State Machine: on_wakeup â†’ on_listening â†’ on_speaking â†’ on_idle     â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ Body Following                                                â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Body yaw syncs with head yaw for natural tracking            â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Extracted from final head pose matrix                        â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GESTURE DETECTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  HaGRID ONNX Models + GestureSmoother                               â”‚  â”‚
â”‚  â”‚  â€¢ 18 gesture classes (call, like, dislike, fist, ok, palm, etc.)    â”‚  â”‚
â”‚  â”‚  â€¢ GestureSmoother with 2-frame confirmation for stable output      â”‚  â”‚
â”‚  â”‚  â€¢ Batch detection: all hands (not just highest confidence)         â”‚  â”‚
â”‚  â”‚  â€¢ Detection frequency: 1 frame interval (high sensitivity)         â”‚  â”‚
â”‚  â”‚  â€¢ Confidence threshold: 0.2 (improved from 0.3)                    â”‚  â”‚
â”‚  â”‚  â€¢ Only runs when face detected (power saving)                       â”‚  â”‚
â”‚  â”‚  â€¢ Real-time state push to Home Assistant                            â”‚  â”‚
â”‚  â”‚  â€¢ No conflicts with face tracking (shared frame, independent)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ESPHOME SERVER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Port 6053 (mDNS auto-discovery)                                      â”‚  â”‚
â”‚  â”‚  â€¢ 54 entities (sensors, controls, media player, camera)              â”‚  â”‚
â”‚  â”‚  â€¢ Voice Assistant pipeline integration                               â”‚  â”‚
â”‚  â”‚  â€¢ Real-time state synchronization                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â”‚ ESPHome Protocol (protobuf)
                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            Home Assistant                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ STT Engine       â”‚  â”‚ Intent Processingâ”‚  â”‚ TTS Engine                 â”‚ â”‚
â”‚  â”‚ (User configured)â”‚  â”‚ (Conversation)   â”‚  â”‚ (User configured)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Software Module Architecture (v0.9.9)

```
reachy_mini_ha_voice/
â”‚
â”œâ”€â”€ main.py                    # Application entry point
â”œâ”€â”€ voice_assistant.py         # Voice assistant service orchestrator
â”œâ”€â”€ satellite.py               # ESPHome protocol handler
â”‚
â”œâ”€â”€ core/                      # Core Infrastructure
â”‚   â”œâ”€â”€ config.py              # Centralized nested configuration
â”‚   â”œâ”€â”€ service_base.py        # SleepAwareService base class
â”‚   â”œâ”€â”€ sleep_manager.py       # Sleep/Wake lifecycle management
â”‚   â”œâ”€â”€ daemon_monitor.py      # Daemon state monitoring
â”‚   â”œâ”€â”€ health_monitor.py      # Service health checking
â”‚   â”œâ”€â”€ memory_monitor.py      # Memory usage monitoring
â”‚   â””â”€â”€ exceptions.py          # Custom exception classes
â”‚
â”œâ”€â”€ motion/                    # Motion Control
â”‚   â”œâ”€â”€ antenna.py             # Antenna animation control
â”‚   â”œâ”€â”€ pose_composer.py       # Pose composition from multiple sources
â”‚   â”œâ”€â”€ gesture_actions.py     # Gesture â†’ Robot action mapping
â”‚   â”œâ”€â”€ smoothing.py           # Motion smoothing algorithms
â”‚   â””â”€â”€ state_machine.py       # Robot state definitions
â”‚
â”œâ”€â”€ vision/                    # Vision Processing
â”‚   â”œâ”€â”€ frame_processor.py     # Adaptive frame rate management
â”‚   â”œâ”€â”€ face_tracking_interpolator.py  # Smooth face tracking
â”‚   â”œâ”€â”€ gesture_smoother.py    # Gesture history tracking and confirmation (v0.9.9)
â”‚   â”œâ”€â”€ gesture_detector.py    # HaGRID gesture detection
â”‚   â””â”€â”€ camera_server.py       # MJPEG camera stream server
â”‚
â”œâ”€â”€ audio/                     # Audio Processing
â”‚   â”œâ”€â”€ microphone.py          # ReSpeaker XVF3800 optimization
â”‚   â””â”€â”€ doa_tracker.py         # Direction of Arrival tracking
â”‚
â”œâ”€â”€ entities/                  # Home Assistant Entities
â”‚   â”œâ”€â”€ entity_factory.py      # Entity creation factory
â”‚   â”œâ”€â”€ entity_keys.py         # Entity key constants
â”‚   â”œâ”€â”€ event_emotion_mapper.py # HA event â†’ Emotion mapping
â”‚   â””â”€â”€ emotion_detector.py    # LLM text emotion detection
â”‚
â””â”€â”€ [Other modules]
    â”œâ”€â”€ movement_manager.py    # 100Hz unified motion control loop
    â”œâ”€â”€ gesture_detector.py     # HaGRID gesture detection (285 lines)
    â”œâ”€â”€ camera_server.py        # MJPEG stream + face tracking (966 lines)
    â”œâ”€â”€ audio_player.py        # TTS + Sendspin playback (624 lines)
    â”œâ”€â”€ entity_registry.py     # ESPHome entity registry
    â””â”€â”€ reachy_controller.py   # Reachy Mini SDK wrapper
```

## Completed Features

### Core Features
- [x] ESPHome protocol server implementation
- [x] mDNS service discovery (auto-discovered by Home Assistant)
- [x] Local wake word detection (microWakeWord)
- [x] Continuous conversation mode (controlled via Home Assistant switch)
- [x] Audio stream transmission to Home Assistant
- [x] TTS audio playback
- [x] Stop word detection

### Reachy Mini Integration
- [x] Use Reachy Mini SDK microphone input
- [x] Use Reachy Mini SDK speaker output
- [x] Head motion control (nod, shake, gaze)
- [x] Antenna animation control
- [x] Voice state feedback actions
- [x] YOLO face tracking (replaces DOA sound source localization)
- [x] 100Hz unified motion control loop

### Application Architecture
- [x] Compliant with Reachy Mini App architecture



## File List

```
reachy_mini_ha_voice/
â”œâ”€â”€ reachy_mini_ha_voice/
â”‚   â”œâ”€â”€ __init__.py             # Package initialization (v0.9.9)
â”‚   â”œâ”€â”€ __main__.py             # Command line entry
â”‚   â”œâ”€â”€ main.py                 # ReachyMiniApp entry
â”‚   â”œâ”€â”€ voice_assistant.py      # Voice assistant service (1066 lines)
â”‚   â”œâ”€â”€ satellite.py            # ESPHome protocol handling (982 lines)
â”‚   â”œâ”€â”€ audio_player.py         # Audio player (TTS + Sendspin) (624 lines)
â”‚   â”œâ”€â”€ camera_server.py        # MJPEG camera stream server + face tracking (966 lines)
â”‚   â”œâ”€â”€ head_tracker.py         # YOLO face detector
â”‚   â”œâ”€â”€ movement_manager.py     # Unified movement manager (100Hz control loop) (1173 lines)
â”‚   â”œâ”€â”€ animation_player.py     # JSON-driven animation system
â”‚   â”œâ”€â”€ speech_sway.py          # Voice-driven head micro-movements
â”‚   â”œâ”€â”€ models.py               # Data models
â”‚   â”œâ”€â”€ entity.py               # ESPHome base entity
â”‚   â”œâ”€â”€ entity_extensions.py    # Extended entity types
â”‚   â”œâ”€â”€ entity_registry.py      # Entity registry (735 lines)
â”‚   â”œâ”€â”€ reachy_controller.py    # Reachy Mini controller wrapper (902 lines)
â”‚   â”œâ”€â”€ gesture_detector.py     # HaGRID gesture detection
â”‚   â”œâ”€â”€ robot_state_monitor.py  # Robot connection state monitoring
â”‚   â”œâ”€â”€ system_diagnostics.py   # System diagnostics (CPU/Memory/Disk)
â”‚   â”œâ”€â”€ emotion_moves.py        # Emotion action playback
â”‚   â”œâ”€â”€ api_server.py           # HTTP API server
â”‚   â”œâ”€â”€ zeroconf.py             # mDNS discovery (ESPHome + Sendspin)
â”‚   â””â”€â”€ util.py                 # Utility functions
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                   # Core infrastructure modules
â”‚   â”‚   â”œâ”€â”€ __init__.py         # Module exports
â”‚   â”‚   â”œâ”€â”€ config.py           # Centralized configuration (368 lines)
â”‚   â”‚   â”œâ”€â”€ daemon_monitor.py   # Daemon state monitoring (329 lines)
â”‚   â”‚   â”œâ”€â”€ service_base.py     # SleepAwareService base class (566 lines)
â”‚   â”‚   â”œâ”€â”€ sleep_manager.py    # Sleep/Wake coordination (269 lines)
â”‚   â”‚   â”œâ”€â”€ health_monitor.py   # Service health checking (309 lines)
â”‚   â”‚   â”œâ”€â”€ memory_monitor.py   # Memory usage monitoring (275 lines)
â”‚   â”‚   â””â”€â”€ exceptions.py       # Custom exception classes (71 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ motion/                 # Motion control modules
â”‚   â”‚   â”œâ”€â”€ __init__.py         # Module exports
â”‚   â”‚   â”œâ”€â”€ antenna.py          # Antenna freeze/unfreeze control (197 lines)
â”‚   â”‚   â”œâ”€â”€ pose_composer.py    # Pose composition utilities (273 lines)
â”‚   â”‚   â”œâ”€â”€ gesture_actions.py  # Gesture to action mapping (383 lines)
â”‚   â”‚   â”œâ”€â”€ smoothing.py        # Smoothing/transition algorithms (198 lines)
â”‚   â”‚   â””â”€â”€ state_machine.py    # State machine definitions (91 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ vision/                 # Vision processing modules
â”‚   â”‚   â”œâ”€â”€ __init__.py         # Module exports
â”‚   â”‚   â”œâ”€â”€ frame_processor.py  # Adaptive frame rate management (268 lines)
â”‚   â”‚   â”œâ”€â”€ face_tracking_interpolator.py  # Face lost interpolation (225 lines)
â”‚   â”‚   â”œâ”€â”€ gesture_smoother.py  # Gesture history tracking (141 lines)
â”‚   â”‚   â”œâ”€â”€ gesture_detector.py  # HaGRID gesture detection (285 lines)
â”‚   â”‚   â””â”€â”€ camera_server.py     # MJPEG camera stream server + face tracking (966 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ audio/                  # Audio processing modules
â”‚   â”‚   â”œâ”€â”€ __init__.py         # Module exports
â”‚   â”‚   â”œâ”€â”€ microphone.py       # ReSpeaker microphone optimization (230 lines)
â”‚   â”‚   â””â”€â”€ doa_tracker.py      # Direction of Arrival tracking (206 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ entities/               # Home Assistant entity modules
â”‚   â”‚   â”œâ”€â”€ __init__.py         # Module exports
â”‚   â”‚   â”œâ”€â”€ entity_factory.py   # Entity factory pattern (516 lines)
â”‚   â”‚   â”œâ”€â”€ entity_keys.py      # Entity key constants (155 lines)
â”‚   â”‚   â”œâ”€â”€ event_emotion_mapper.py  # HA event to emotion mapping (341 lines)
â”‚   â”‚   â””â”€â”€ emotion_detector.py # LLM emotion keyword detection (119 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ animations/             # Animation definitions
â”‚   â”‚   â”œâ”€â”€ conversation_animations.json  # Conversation state animations
â”‚   â”‚   â””â”€â”€ emotion_keywords.json         # Emotion keyword mapping (280+ keywords)
â”‚   â”‚
â”‚   â””â”€â”€ wakewords/              # Wake word models
â”‚       â”œâ”€â”€ okay_nabu.json/.tflite
â”‚       â”œâ”€â”€ hey_jarvis.json/.tflite (openWakeWord)
â”‚       â”œâ”€â”€ alexa.json/.tflite
â”‚       â”œâ”€â”€ hey_luna.json/.tflite
â”‚       â””â”€â”€ stop.json/.tflite   # Stop word detection
â”‚
â”œâ”€â”€ sounds/                     # Sound effect files (auto-download)
â”‚   â”œâ”€â”€ wake_word_triggered.flac
â”‚   â””â”€â”€ timer_finished.flac
â”œâ”€â”€ pyproject.toml              # Project configuration
â”œâ”€â”€ README.md                   # Documentation
â”œâ”€â”€ changelog.json              # Version changelog
â””â”€â”€ PROJECT_PLAN.md             # Project plan
```

## Dependencies

```toml
dependencies = [
    # Reachy Mini SDK (provides audio via media system)
    "reachy-mini",

    # Audio processing (fallback when not on Reachy Mini)
    "sounddevice>=0.5.0",
    "soundfile>=0.13.0",
    "numpy>=2.0.0",

    # Camera streaming
    "opencv-python>=4.10.0",

    # Wake word detection (local)
    "pymicro-wakeword>=2.0.0,<3.0.0",
    "pyopen-wakeword>=1.0.0,<2.0.0",

    # ESPHome protocol (communication with Home Assistant)
    "aioesphomeapi>=43.10.1",
    "zeroconf>=0.140.0",

    # Motion control (head movements)
    "scipy>=1.14.0",

    # Face tracking (YOLO-based head detection)
    "ultralytics>=8.3.0",
    "supervision>=0.25.0",
    "huggingface_hub>=0.27.0",

    # Sendspin synchronized audio (optional, for multi-room playback)
    "aiosendspin>=2.0.1",

    # Gesture detection (ONNX runtime for HaGRID models)
    "onnxruntime>=1.18.0",
]
```

## Usage Flow

1. **Install App**
   - Install `reachy_mini_ha_voice` from Reachy Mini App Store

2. **Start App**
   - App auto-starts ESPHome server (port 6053)
   - Auto-downloads required models and sounds

3. **Connect Home Assistant**
   - Home Assistant auto-discovers device (mDNS)
   - Or manually add: Settings â†’ Devices & Services â†’ Add Integration â†’ ESPHome

4. **Use Voice Assistant**
   - Say "Okay Nabu" to wake
   - Speak command
   - Reachy Mini provides motion feedback

## ESPHome Entity Planning

Based on deep analysis of Reachy Mini SDK, the following entities are exposed to Home Assistant:

### Implemented Entities

| Entity Type | Name | Description |
|-------------|------|-------------|
| Media Player | `media_player` | Audio playback control |
| Voice Assistant | `voice_assistant` | Voice assistant pipeline |

### Implemented Control Entities (Read/Write)

#### Phase 1-3: Basic Controls and Pose

| ESPHome Entity Type | Name | SDK API | Range/Options | Description |
|---------------------|------|---------|---------------|-------------|
| `Number` | `speaker_volume` | `AudioPlayer.set_volume()` | 0-100 | Speaker volume |
| `Select` | `motor_mode` | `set_motor_control_mode()` | enabled/disabled/gravity_compensation | Motor mode selection |
| `Switch` | `motors_enabled` | `enable_motors()` / `disable_motors()` | on/off | Motor torque switch |
| `Button` | `wake_up` | `mini.wake_up()` | - | Wake robot action |
| `Button` | `go_to_sleep` | `mini.goto_sleep()` | - | Sleep robot action |
| `Number` | `head_x` | `goto_target(head=...)` | Â±50mm | Head X position control |
| `Number` | `head_y` | `goto_target(head=...)` | Â±50mm | Head Y position control |
| `Number` | `head_z` | `goto_target(head=...)` | Â±50mm | Head Z position control |
| `Number` | `head_roll` | `goto_target(head=...)` | -40Â° ~ +40Â° | Head roll angle control |
| `Number` | `head_pitch` | `goto_target(head=...)` | -40Â° ~ +40Â° | Head pitch angle control |
| `Number` | `head_yaw` | `goto_target(head=...)` | -180Â° ~ +180Â° | Head yaw angle control |
| `Number` | `body_yaw` | `goto_target(body_yaw=...)` | -160Â° ~ +160Â° | Body yaw angle control |
| `Number` | `antenna_left` | `goto_target(antennas=...)` | -90Â° ~ +90Â° | Left antenna angle control |
| `Number` | `antenna_right` | `goto_target(antennas=...)` | -90Â° ~ +90Â° | Right antenna angle control |

#### Phase 4: Gaze Control

| ESPHome Entity Type | Name | SDK API | Range/Options | Description |
|---------------------|------|---------|---------------|-------------|
| `Number` | `look_at_x` | `look_at_world(x, y, z)` | World coordinates | Gaze point X coordinate |
| `Number` | `look_at_y` | `look_at_world(x, y, z)` | World coordinates | Gaze point Y coordinate |
| `Number` | `look_at_z` | `look_at_world(x, y, z)` | World coordinates | Gaze point Z coordinate |


### Implemented Sensor Entities (Read-only)

#### Phase 1 & 5: Basic Status and Audio Sensors

| ESPHome Entity Type | Name | SDK API | Description |
|---------------------|------|---------|-------------|
| `Text Sensor` | `daemon_state` | `DaemonStatus.state` | Daemon status |
| `Binary Sensor` | `backend_ready` | `backend_status.ready` | Backend ready status |
| `Text Sensor` | `error_message` | `DaemonStatus.error` | Current error message |
| `Sensor` | `doa_angle` | `DoAInfo.angle` | Sound source direction angle (Â°) |
| `Binary Sensor` | `speech_detected` | `DoAInfo.speech_detected` | Speech detection status |

#### Phase 6: Diagnostic Information

| ESPHome Entity Type | Name | SDK API | Description |
|---------------------|------|---------|-------------|
| `Sensor` | `control_loop_frequency` | `control_loop_stats` | Control loop frequency (Hz) |
| `Text Sensor` | `sdk_version` | `DaemonStatus.version` | SDK version |
| `Text Sensor` | `robot_name` | `DaemonStatus.robot_name` | Robot name |
| `Binary Sensor` | `wireless_version` | `DaemonStatus.wireless_version` | Wireless version flag |
| `Binary Sensor` | `simulation_mode` | `DaemonStatus.simulation_enabled` | Simulation mode flag |
| `Text Sensor` | `wlan_ip` | `DaemonStatus.wlan_ip` | Wireless IP address |

#### Phase 7: IMU Sensors (Wireless version only)

| ESPHome Entity Type | Name | SDK API | Description |
|---------------------|------|---------|-------------|
| `Sensor` | `imu_accel_x` | `mini.imu["accelerometer"][0]` | X-axis acceleration (m/sÂ²) |
| `Sensor` | `imu_accel_y` | `mini.imu["accelerometer"][1]` | Y-axis acceleration (m/sÂ²) |
| `Sensor` | `imu_accel_z` | `mini.imu["accelerometer"][2]` | Z-axis acceleration (m/sÂ²) |
| `Sensor` | `imu_gyro_x` | `mini.imu["gyroscope"][0]` | X-axis angular velocity (rad/s) |
| `Sensor` | `imu_gyro_y` | `mini.imu["gyroscope"][1]` | Y-axis angular velocity (rad/s) |
| `Sensor` | `imu_gyro_z` | `mini.imu["gyroscope"][2]` | Z-axis angular velocity (rad/s) |
| `Sensor` | `imu_temperature` | `mini.imu["temperature"]` | IMU temperature (Â°C) |

#### Phase 8-12: Extended Features

| ESPHome Entity Type | Name | Description |
|---------------------|------|-------------|
| `Select` | `emotion` | Emotion selector (Happy/Sad/Angry/Fear/Surprise/Disgust) |
| `Number` | `microphone_volume` | Microphone volume (0-100%) |
| `Camera` | `camera` | ESPHome Camera entity (live preview) |
| `Number` | `led_brightness` | LED brightness (0-100%) |
| `Select` | `led_effect` | LED effect (off/solid/breathing/rainbow/doa) |
| `Number` | `led_color_r` | LED red component (0-255) |
| `Number` | `led_color_g` | LED green component (0-255) |
| `Number` | `led_color_b` | LED blue component (0-255) |
| `Switch` | `agc_enabled` | Auto gain control switch |
| `Number` | `agc_max_gain` | AGC max gain (0-30 dB) |
| `Number` | `noise_suppression` | Noise suppression level (0-100%) |
| `Binary Sensor` | `echo_cancellation_converged` | Echo cancellation convergence status |

> **Note**: Head position (x/y/z) and angles (roll/pitch/yaw), body yaw, antenna angles are all **controllable** entities,
> using `Number` type for bidirectional control. Call `goto_target()` when setting new values, call `get_current_head_pose()` etc. when reading current values.

### Implementation Priority

1. **Phase 1 - Basic Status and Volume** (High Priority) âœ… **Completed**
   - [x] `daemon_state` - Daemon status sensor
   - [x] `backend_ready` - Backend ready status
   - [x] `error_message` - Error message
   - [x] `speaker_volume` - Speaker volume control

2. **Phase 2 - Motor Control** (High Priority) âœ… **Completed**
   - [x] `motors_enabled` - Motor switch
   - [x] `motor_mode` - Motor mode selection (enabled/disabled/gravity_compensation)
   - [x] `wake_up` / `go_to_sleep` - Wake/sleep buttons

3. **Phase 3 - Pose Control** (Medium Priority) âœ… **Completed**
   - [x] `head_x/y/z` - Head position control
   - [x] `head_roll/pitch/yaw` - Head angle control
   - [x] `body_yaw` - Body yaw angle control
   - [x] `antenna_left/right` - Antenna angle control

4. **Phase 4 - Gaze Control** (Medium Priority) âœ… **Completed**
   - [x] `look_at_x/y/z` - Gaze point coordinate control

5. **Phase 5 - DOA (Direction of Arrival)** âœ… **Re-added for wakeup turn-to-sound**
   - [x] `doa_angle` - Sound source direction (degrees, 0-180Â°, where 0Â°=left, 90Â°=front, 180Â°=right)
   - [x] `speech_detected` - Speech detection status
   - [x] Turn-to-sound at wakeup (robot turns toward speaker when wake word detected)
   - [x] Direction correction: `yaw = Ï€/2 - doa` (fixed left/right inversion)
   - Note: DOA only read once at wakeup to avoid daemon pressure; face tracking takes over after

6. **Phase 6 - Diagnostic Information** (Low Priority) âœ… **Completed**
   - [x] `control_loop_frequency` - Control loop frequency
   - [x] `sdk_version` - SDK version
   - [x] `robot_name` - Robot name
   - [x] `wireless_version` - Wireless version flag
   - [x] `simulation_mode` - Simulation mode flag
   - [x] `wlan_ip` - Wireless IP address

7. **Phase 7 - IMU Sensors** (Optional, wireless version only) âœ… **Completed**
   - [x] `imu_accel_x/y/z` - Accelerometer
   - [x] `imu_gyro_x/y/z` - Gyroscope
   - [x] `imu_temperature` - IMU temperature

8. **Phase 8 - Emotion Control** âœ… **Completed**
   - [x] `emotion` - Emotion selector (Happy/Sad/Angry/Fear/Surprise/Disgust)

9. **Phase 9 - Audio Control** âœ… **Completed**
   - [x] `microphone_volume` - Microphone volume control (0-100%)

10. **Phase 10 - Camera Integration** âœ… **Completed**
    - [x] `camera` - ESPHome Camera entity (live preview)

11. **Phase 11 - LED Control** âŒ **Disabled (LEDs hidden inside robot)**
    - [ ] `led_brightness` - LED brightness (0-100%) - Commented out
    - [ ] `led_effect` - LED effect (off/solid/breathing/rainbow/doa) - Commented out
    - [ ] `led_color_r/g/b` - LED RGB color (0-255) - Commented out

12. **Phase 12 - Audio Processing Parameters** âœ… **Completed**
    - [x] `agc_enabled` - Auto gain control switch
    - [x] `agc_max_gain` - AGC max gain (0-30 dB)
    - [x] `noise_suppression` - Noise suppression level (0-100%)
    - [x] `echo_cancellation_converged` - Echo cancellation convergence status (read-only)

13. **Phase 13 - Sendspin Audio Playback Support** âœ… **Completed**
    - [x] `sendspin_enabled` - Sendspin switch (Switch)
    - [x] `sendspin_url` - Sendspin server URL (Text Sensor)
    - [x] `sendspin_connected` - Sendspin connection status (Binary Sensor)
    - [x] AudioPlayer integrates aiosendspin library
    - [x] TTS audio sent to both local speaker and Sendspin server

14. **Phase 22 - Gesture Detection** âœ… **Completed (v0.9.9: Optimized with GestureSmoother)**
    - [x] `gesture_detected` - Detected gesture name (Text Sensor)
    - [x] `gesture_confidence` - Gesture detection confidence % (Sensor)
    - [x] HaGRID ONNX models: hand_detector.onnx + crops_classifier.onnx
    - [x] Real-time state push to Home Assistant
    - [x] GestureSmoother with 2-frame confirmation mechanism for stable output
    - [x] Batch detection: returns all detected hands (not just highest confidence)
    - [x] Detection frequency: 1 frame interval (improved from 3 frames)
    - [x] Confidence threshold: 0.2 (lowered from 0.3 for improved sensitivity)
    - [x] No conflicts with face tracking (shared frame, independent processing)
    - [x] 18 supported gestures:
      | Gesture | Emoji | Gesture | Emoji |
      |---------|-------|---------|-------|
      | call | ðŸ¤™ | like | ðŸ‘ |
      | dislike | ðŸ‘Ž | mute | ðŸ¤« |
      | fist | âœŠ | ok | ðŸ‘Œ |
      | four | ðŸ–ï¸ | one | â˜ï¸ |
      | palm | âœ‹ | peace | âœŒï¸ |
      | peace_inverted | ðŸ”»âœŒï¸ | rock | ðŸ¤˜ |
      | stop | ðŸ›‘ | stop_inverted | ðŸ”»ðŸ›‘ |
      | three | 3ï¸âƒ£ | three2 | ðŸ¤Ÿ |
      | two_up | âœŒï¸â˜ï¸ | two_up_inverted | ðŸ”»âœŒï¸â˜ï¸ |

15. **Phase 24 - System Diagnostics** âœ… **Completed**
    - [x] `cpu_percent` - CPU usage percentage (Sensor, diagnostic)
    - [x] `cpu_temperature` - CPU temperature in Celsius (Sensor, diagnostic)
    - [x] `memory_percent` - Memory usage percentage (Sensor, diagnostic)
    - [x] `memory_used_gb` - Used memory in GB (Sensor, diagnostic)
    - [x] `disk_percent` - Disk usage percentage (Sensor, diagnostic)
    - [x] `disk_free_gb` - Free disk space in GB (Sensor, diagnostic)
    - [x] `uptime_hours` - System uptime in hours (Sensor, diagnostic)
    - [x] `process_cpu_percent` - This process CPU usage (Sensor, diagnostic)
    - [x] `process_memory_mb` - This process memory in MB (Sensor, diagnostic)

---

## ðŸŽ‰ Phase 1-13 + Phase 22 + Phase 24 Entities Completed!

**Total Completed: 54 entities**
- Phase 1: 4 entities (Basic status and volume)
- Phase 2: 4 entities (Motor control)
- Phase 3: 9 entities (Pose control)
- Phase 4: 3 entities (Gaze control)
- Phase 5: 2 entities (Audio sensors)
- Phase 6: 6 entities (Diagnostic information)
- Phase 7: 7 entities (IMU sensors)
- Phase 8: 1 entity (Emotion control)
- Phase 9: 1 entity (Microphone volume)
- Phase 10: 1 entity (Camera)
- Phase 11: 0 entities (LED control - Disabled)
- Phase 12: 4 entities (Audio processing parameters)
- Phase 13: 3 entities (Sendspin audio output)
- Phase 22: 2 entities (Gesture detection)
- Phase 24: 9 entities (System diagnostics)


---

## ðŸš€ Voice Assistant Enhancement Features Implementation Status

### Phase 14 - Emotion Action Feedback System (Enhanced) âœ…

**Implementation Status**: Full keyword-based emotion detection implemented with 280+ Chinese/English keywords mapped to 35 emotion categories

**Implemented Features**:
- âœ… Phase 8 Emotion Selector entity (`emotion`)
- âœ… Basic emotion action playback API (`_play_emotion`)
- âœ… Emotion mapping: Happy/Sad/Angry/Fear/Surprise/Disgust
- âœ… Integration with HuggingFace action library (`pollen-robotics/reachy-mini-emotions-library`)
- âœ… SpeechSway system for natural head micro-movements during conversation (non-blocking)
- âœ… Tap detection disabled during emotion playback (polls daemon API for completion)
- âœ… **NEW (v0.8.0)**: Comprehensive emotion keyword detection from conversation text
- âœ… **NEW (v0.8.0)**: 280+ Chinese and English keywords mapped to 35 emotion categories
- âœ… **NEW (v0.8.0)**: Auto-trigger expressions based on text patterns in LLM responses

**Emotion Keyword Categories (v0.8.0)**:

| Expression ID | Category | Chinese Keywords | English Keywords |
|---------------|----------|------------------|------------------|
| `cheerful1` | Happy | å¤ªæ£’äº†ã€å¼€å¿ƒã€é«˜å…´ | great, awesome, happy |
| `laughing1` | Laughing | å“ˆå“ˆã€ç¬‘æ­»ã€å¥½ç¬‘ | haha, lol, funny |
| `enthusiastic1` | Excited | å…´å¥‹ã€æ¿€åŠ¨ã€è€¶ | excited, yay, cool |
| `amazed1` | Amazed | ç¥žå¥‡ã€åŽ‰å®³ã€ç‰› | amazing, incredible |
| `surprised1` | Surprised | å“‡ã€å¤©å•Šã€çœŸçš„å— | wow, omg, really |
| `loving1` | Love | çˆ±ã€å–œæ¬¢ã€å¯çˆ± | love, cute, adore |
| `grateful1` | Grateful | è°¢è°¢ã€æ„Ÿè°¢ | thanks, appreciate |
| `welcoming1` | Welcome | æ¬¢è¿Žã€ä½ å¥½ | hello, welcome |
| `helpful1` | Helpful | å½“ç„¶ã€å¥½çš„ã€æ²¡é—®é¢˜ | sure, of course |
| `curious1` | Curious | å¥½å¥‡ã€æœ‰è¶£ | curious, interesting |
| `thoughtful1` | Thinking | å—¯ã€è®©æˆ‘æƒ³æƒ³ | hmm, let me think |
| `sad1` | Sad | éš¾è¿‡ã€ä¼¤å¿ƒã€å¯æƒœ | sad, unfortunately |
| `oops1` | Oops | æŠ±æ­‰ã€ç³Ÿç³•ã€å“Žå‘€ | sorry, oops |
| `confused1` | Confused | å›°æƒ‘ã€æžä¸æ‡‚ | confused, puzzled |
| `fear1` | Fear | å®³æ€•ã€å¯æ€• | afraid, scared |
| `rage1` | Angry | ç”Ÿæ°”ã€æ„¤æ€’ | angry, mad |
| `yes1` | Yes | æ˜¯çš„ã€å¯¹ã€æ²¡é”™ | yes, correct |
| `no1` | No | ä¸æ˜¯ã€ä¸è¡Œ | no, wrong |
| ... | ... | ... | ... |

**Design Decisions**:
- ðŸŽ¯ No auto-play of full emotion actions during conversation to avoid blocking
- ðŸŽ¯ Use voice-driven head sway (SpeechSway) for natural motion feedback
- ðŸŽ¯ Emotion actions retained as manual trigger feature via ESPHome entity
- ðŸŽ¯ Tap detection waits for actual move completion via `/api/move/running` polling
- ðŸŽ¯ **NEW**: Keyword detection is case-insensitive and configurable via JSON

**Partially Implemented**:
- ðŸŸ¡ Intent recognition and emotion matching (basic keyword matching implemented)
- âŒ Dance action library integration
- âŒ Context awareness (e.g., weather query - sunny plays happy, rainy plays sad)

**Code Locations**:
- `animations/emotion_keywords.json` - **NEW**: Emotion keyword mapping configuration (280+ keywords)
- `entity_registry.py:633-658` - Emotion Selector entity
- `satellite.py:_load_emotion_keywords()` - Load emotion keywords from JSON
- `satellite.py:_detect_and_play_emotion()` - Auto-detect emotions from text
- `satellite.py:_play_emotion()` - Emotion playback with move UUID tracking
- `satellite.py:_wait_for_move_completion()` - Polls daemon API for move completion
- `motion.py:132-156` - Conversation start motion control (uses SpeechSway)
- `movement_manager.py:541-595` - Move queue management (allows SpeechSway overlay)

**Actual Behavior**:

| Voice Assistant Event | Actual Action | Implementation Status |
|----------------------|---------------|----------------------|
| Wake word detected | Turn toward sound source + nod confirmation | âœ… Implemented |
| Conversation start | Voice-driven head micro-movements (SpeechSway) | âœ… Implemented |
| During conversation | Continuous voice-driven micro-movements + breathing animation | âœ… Implemented |
| Conversation end | Return to neutral position + breathing animation | âœ… Implemented |
| Manual emotion trigger | Play via ESPHome `emotion` entity | âœ… Implemented |

**Technical Details**:
```python
# motion.py - Use SpeechSway instead of full emotion actions during conversation
def on_speaking_start(self):
    self._is_speaking = True
    self._movement_manager.set_state(RobotState.SPEAKING)
    # SpeechSway automatically generates natural head micro-movements based on audio loudness
    # No full emotion actions played to avoid blocking conversation experience

# movement_manager.py - Motion layering system
# 1. Move queue (emotion actions) - Sets base pose
# 2. Action (nod/shake etc.) - Overlays on base pose
# 3. SpeechSway - Voice-driven micro-movements, can coexist with Move
# 4. Breathing - Idle breathing animation
```

**Original Plan** (Decided not to implement to avoid blocking conversation):

| Voice Assistant Event | Original Planned Action | Reason Not Implemented |
|----------------------|------------------------|------------------------|
| Positive response received | Play "happy" action | Full action would block conversation fluency |
| Negative response received | Play "sad" action | Full action would block conversation fluency |
| Play music/entertainment | Play "dance" action | Full action would block conversation fluency |
| Timer completed | Play "alert" action | Full action would block conversation fluency |
| Error/cannot understand | Play "confused" action | Full action would block conversation fluency |

**Manual Emotion Trigger Example**:
```yaml
# Home Assistant automation example - Manual emotion trigger
automation:
  - alias: "Reachy Good Morning Greeting"
    trigger:
      - platform: time
        at: "07:00:00"
    action:
      - service: select.select_option
        target:
          entity_id: select.reachy_mini_emotion
        data:
          option: "Happy"
```

### Phase 15 - Face Tracking (Complements DOA Turn-to-Sound) âœ… **Completed**

**Goal**: Implement natural face tracking so robot looks at speaker during conversation.

**Design Decision**:
- âœ… DOA (Direction of Arrival): Used once at wakeup to turn toward sound source
- âœ… YOLO face detection: Takes over after initial turn for continuous tracking
- âœ… Body follows head rotation: Body yaw automatically syncs with head yaw for natural tracking
- Reason: DOA provides quick initial orientation, face tracking provides accurate continuous tracking, body following enables natural whole-body tracking similar to human behavior

**Wakeup Turn-to-Sound Flow**:
1. Wake word detected â†’ Read DOA angle once (avoid daemon pressure)
2. If DOA angle > 10Â°: Turn head toward sound source (80% of angle, conservative)
3. Face tracking takes over for continuous tracking during conversation

**Implemented Features**:

| Feature | Description | Implementation Location | Status |
|---------|-------------|------------------------|--------|
| DOA turn-to-sound | Turn toward speaker at wakeup | `satellite.py:_turn_to_sound_source()` | âœ… Implemented |
| YOLO face detection | Uses `AdamCodd/YOLOv11n-face-detection` model | `head_tracker.py` | âœ… Implemented |
| Adaptive frame rate tracking | 15fps during conversation, 2fps when idle without face | `camera_server.py` | âœ… Implemented |
| look_at_image() | Calculate target pose from face position | `camera_server.py` | âœ… Implemented |
| Smooth return to neutral | Smooth return within 1 second after face lost | `camera_server.py` | âœ… Implemented |
| face_tracking_offsets | As secondary pose overlay to motion control | `movement_manager.py` | âœ… Implemented |
| Body follows head rotation | Body yaw syncs with head yaw extracted from final pose matrix | `movement_manager.py:_compose_final_pose()` | âœ… Implemented (v0.8.3) |
| DOA entities | `doa_angle` and `speech_detected` exposed to Home Assistant | `entity_registry.py` | âœ… Implemented |
| face_detected entity | Binary sensor for face detection state | `entity_registry.py` | âœ… Implemented |
| Model download retry | 3 retries, 5 second interval | `head_tracker.py` | âœ… Implemented |
| Conversation mode integration | Auto-switch tracking frequency on voice assistant state change | `satellite.py` | âœ… Implemented |

**Resource Optimization (v0.5.1, updated v0.6.2)**:
- During conversation (listening/thinking/speaking): High-frequency tracking 15fps
- Idle with face detected: High-frequency tracking 15fps
- Idle without face for 5s: Low-power mode 2fps
- Idle without face for 30s: Ultra-low power mode 0.5fps (every 2 seconds)
- Gesture detection only runs when face detected recently (within 5s)
- Immediately restore high-frequency tracking when face detected

**Code Locations**:
- `satellite.py:_turn_to_sound_source()` - DOA turn-to-sound at wakeup
- `head_tracker.py` - YOLO face detector (`HeadTracker` class)
- `camera_server.py:_capture_frames()` - Adaptive frame rate face tracking
- `camera_server.py:set_conversation_mode()` - Conversation mode switch API
- `satellite.py:_set_conversation_mode()` - Voice assistant state integration
- `movement_manager.py:set_face_tracking_offsets()` - Face tracking offset API
- `movement_manager.py:_compose_final_pose()` - Body yaw follows head yaw (v0.8.3)

**Technical Details**:
```python
# camera_server.py - Adaptive frame rate face tracking
class MJPEGCameraServer:
    def __init__(self):
        self._fps_high = 15  # During conversation/face detected
        self._fps_low = 2    # Idle without face (5-30s)
        self._fps_idle = 0.5 # Ultra-low power (>30s without face)
        self._low_power_threshold = 5.0   # 5s without face switches to low power
        self._idle_threshold = 30.0       # 30s without face switches to idle mode

    def _should_run_ai_inference(self, current_time):
        # Conversation mode: Always high-frequency tracking
        if self._in_conversation:
            return True
        # High-frequency mode: Track every frame
        if self._current_fps == self._fps_high:
            return True
        # Low/idle power mode: Periodic detection
        return time.since_last_check >= 1/self._current_fps

# satellite.py - Voice assistant state integration
def _reachy_on_listening(self):
    self._set_conversation_mode(True)  # Start conversation, high-frequency tracking

def _reachy_on_idle(self):
    self._set_conversation_mode(False)  # End conversation, adaptive tracking

# movement_manager.py - Body follows head rotation (v0.8.3)
# This enables natural body rotation when tracking faces, similar to how
# the reference project's sweep_look tool synchronizes body_yaw with head_yaw.
def _compose_final_pose(self) -> Tuple[np.ndarray, Tuple[float, float], float]:
    # ... compose head pose from all motion sources ...

    # Extract yaw from final head pose rotation matrix
    # The rotation matrix uses xyz euler convention
    final_rotation = R.from_matrix(final_head[:3, :3])
    _, _, final_head_yaw = final_rotation.as_euler('xyz')

    # Body follows head yaw directly
    # SDK's automatic_body_yaw (inverse_kinematics_safe) only handles collision
    # prevention by clamping relative angle to max 65Â°, not active following
    body_yaw = final_head_yaw

    return final_head, (antenna_right, antenna_left), body_yaw
```

**Body Following Head Rotation (v0.8.3)**:
- SDK's `automatic_body_yaw` is only **collision protection**, not active body following
- The `inverse_kinematics_safe` function with `max_relative_yaw=65Â°` only prevents head-body collision
- To enable natural body following, `body_yaw` must be explicitly set to match `head_yaw`
- Body yaw is extracted from final head pose matrix using scipy's `R.from_matrix().as_euler('xyz')`
- This matches the reference project's `sweep_look.py` behavior where `target_body_yaw = head_yaw`


### Phase 16 - Cartoon Style Motion Mode (Partial) ðŸŸ¡

**Goal**: Use SDK interpolation techniques for more expressive robot movements.

**SDK Support**: `InterpolationTechnique` enum
- `LINEAR` - Linear, mechanical feel
- `MIN_JERK` - Minimum jerk, natural and smooth (default)
- `EASE_IN_OUT` - Ease in-out, elegant
- `CARTOON` - Cartoon style, with bounce effect, lively and cute

**Implemented Features**:
- âœ… 100Hz unified control loop (`movement_manager.py`) - Restored to 100Hz after daemon update
- âœ… JSON-driven animation system (`AnimationPlayer`) - Inspired by SimpleDances project
- âœ… Conversation state animations (idle/listening/thinking/speaking)
- âœ… Pose change detection - Only send commands on significant changes (threshold 0.005)
- âœ… State query caching - 2s TTL, reduces daemon load
- âœ… Smooth interpolation (ease in-out curve)
- âœ… Command queue mode - Thread-safe external API
- âœ… Error throttling - Prevents log explosion
- âœ… Connection health monitoring - Auto-detect and recover from connection loss

**Animation System (v0.5.13)**:
- `AnimationPlayer` class loads animations from `conversation_animations.json`
- Each animation defines: pitch/yaw/roll amplitudes, position offsets, antenna movements, frequency
- Smooth transitions between animations (configurable duration)
- State-to-animation mapping: idleâ†’idle, listeningâ†’listening, thinkingâ†’thinking, speakingâ†’speaking

**Not Implemented**:
- âŒ Dynamic interpolation technique switching (CARTOON/EASE_IN_OUT etc.)
- âŒ Exaggerated cartoon bounce effects

**Code Locations**:
- `animation_player.py` - AnimationPlayer class
- `animations/conversation_animations.json` - Animation definitions
- `movement_manager.py` - 100Hz control loop with animation integration

**Scene Implementation Status**:

| Scene | Recommended Interpolation | Effect | Status |
|-------|--------------------------|--------|--------|
| Wake nod | `CARTOON` | Lively bounce effect | âŒ Not implemented |
| Thinking head up | `EASE_IN_OUT` | Elegant transition | âœ… Implemented (smooth interpolation) |
| Speaking micro-movements | `MIN_JERK` | Natural and fluid | âœ… Implemented (SpeechSway) |
| Error head shake | `CARTOON` | Exaggerated denial | âŒ Not implemented |
| Return to neutral | `MIN_JERK` | Smooth return | âœ… Implemented |
| Idle breathing | - | Subtle sense of life | âœ… Implemented (BreathingAnimation) |

### Phase 17 - Antenna Sync Animation During Speech (Completed) âœ…

**Goal**: Antennas sway with audio rhythm during TTS playback, simulating "speaking" effect.

**Implemented Features**:
- âœ… JSON-driven animation system with antenna movements
- âœ… Different antenna patterns: "both" (sync), "wiggle" (opposite phase)
- âœ… State-specific antenna animations (listening/thinking/speaking)
- âœ… Smooth transitions between animation states

**Code Locations**:
- `animation_player.py` - AnimationPlayer with antenna offset calculation
- `animations/conversation_animations.json` - Antenna amplitude and pattern definitions
- `movement_manager.py` - Antenna offset composition in final pose

### Phase 18 - Visual Gaze Interaction (Not Implemented) âŒ

**Goal**: Use camera to detect faces for eye contact.

**SDK Support**:
- `look_at_image(u, v)` - Look at point in image
- `look_at_world(x, y, z)` - Look at world coordinate point
- `media.get_frame()` - Get camera frame (âœ… Already implemented in `camera_server.py:146`)

**Not Implemented Features**:

| Feature | Description | Status |
|---------|-------------|--------|
| Face detection | Use OpenCV/MediaPipe to detect faces | âŒ Not implemented |
| Eye tracking | Look at speaker's face during conversation | âŒ Not implemented |
| Multi-person switching | When multiple people detected, look at current speaker | âŒ Not implemented |
| Idle scanning | Randomly look around when idle | âŒ Not implemented |

### Phase 19 - Gravity Compensation Interactive Mode (Partial) ðŸŸ¡

**Goal**: Allow users to physically touch and guide robot head for "teaching" style interaction.

**SDK Support**: `enable_gravity_compensation()` - Motors enter gravity compensation mode, can be manually moved

**Implemented Features**:
- âœ… Gravity compensation mode switch (`motor_mode` Select entity, option "gravity_compensation")
- âœ… `reachy_controller.py:236-237` - Gravity compensation API call

**Not Implemented**:
- âŒ Teaching mode - Record motion trajectory
- âŒ Save/playback custom actions
- âŒ Voice command triggered teaching flow

**Application Scenarios**:
- âŒ User says "Let me teach you a move" â†’ Enter gravity compensation mode
- âŒ User manually moves head â†’ Record motion trajectory
- âŒ User says "Remember this" â†’ Save action
- âŒ User says "Do that action again" â†’ Playback recorded action

### Phase 20 - Environment Awareness Response (Partial) ðŸŸ¡

**Goal**: Use IMU sensors to sense environment changes and respond.

**SDK Support**:
- âœ… `mini.imu["accelerometer"]` - Accelerometer (Phase 7 implemented as entity)
- âœ… `mini.imu["gyroscope"]` - Gyroscope (Phase 7 implemented as entity)

**Implemented Features**:

| Feature | Description | Status |
|---------|-------------|--------|
| Continuous conversation | Controlled via Home Assistant switch | âœ… Implemented |
| IMU sensor entities | Accelerometer and gyroscope exposed to HA | âœ… Implemented |

> **Note**: Tap-to-wake feature was removed in v0.5.16 due to false triggers from robot movement. Continuous conversation is now controlled via Home Assistant switch.

**Not Implemented**:

| Detection Event | Response Action | Status |
|-----------------|-----------------|--------|
| Being shaken | Play dizzy action + voice "Don't shake me~" | âŒ Not implemented |
| Tilted/fallen | Play help action + voice "I fell, help me" | âŒ Not implemented |
| Long idle | Enter sleep animation | âŒ Not implemented |

### Phase 21 - Home Assistant Scene Integration (Not Implemented) âŒ

**Goal**: Trigger robot actions based on Home Assistant scenes/automations.

**Implementation**: Via ESPHome service calls

**Not Implemented Scenes**:

| HA Scene | Robot Response | Status |
|----------|----------------|--------|
| Good morning scene | Play wake action + "Good morning!" | âŒ Not implemented |
| Good night scene | Play sleep action + "Good night~" | âŒ Not implemented |
| Someone home | Turn toward door + wave + "Welcome home!" | âŒ Not implemented |
| Doorbell rings | Turn toward door + alert action | âŒ Not implemented |
| Play music | Sway with music rhythm | âŒ Not implemented |


---

## ðŸ“Š Feature Implementation Summary

### âœ… Completed Features

#### Core Voice Assistant (Phase 1-12)
- **54 ESPHome entities** - All implemented (Phase 11 LED disabled)
- **Basic voice interaction** - Wake word detection (microWakeWord/openWakeWord), STT/TTS integration
- **Motion feedback** - Nod, shake, gaze and other basic actions
- **Audio processing** - AGC, noise suppression, echo cancellation
- **Camera stream** - MJPEG live preview with ESPHome Camera entity

#### Extended Features (Phase 13-22)
- **Phase 13** âœ… - Sendspin multi-room audio support
- **Phase 14** âœ… - Emotion keyword detection (280+ keywords, 35 categories)
- **Phase 15** âœ… - Face tracking with body following (DOA + YOLO + body_yaw sync)
- **Phase 16** âœ… - JSON-driven animation system (100Hz control loop)
- **Phase 17** âœ… - Antenna sync animation during speech
- **Phase 22** âœ… - Gesture detection (HaGRID ONNX, 18 gestures)

### ðŸŸ¡ Partially Implemented Features

- **Phase 19** - Gravity compensation mode switch (teaching flow not implemented)
- **Phase 20** - IMU sensor entities (trigger logic not implemented)

### âŒ Not Implemented Features

- **Phase 18** - Visual gaze interaction (eye contact with multiple people)
- **Phase 21** - Home Assistant scene integration (morning/night routines)

---

## Feature Priority Summary (Updated v0.9.5)

### Completed âœ…
- âœ… **Phase 1-12**: Core ESPHome entities and voice assistant
- âœ… **Phase 13**: Sendspin audio playback
- âœ… **Phase 14**: Emotion keyword detection and auto-trigger
- âœ… **Phase 15**: Face tracking with body following
- âœ… **Phase 16**: JSON-driven animation system
- âœ… **Phase 17**: Antenna sync animation
- âœ… **Phase 22**: Gesture detection
- âœ… **Phase 24**: System diagnostics (psutil-based)

### Partial ðŸŸ¡
- ðŸŸ¡ **Phase 19**: Gravity compensation mode (teaching flow pending)
- ðŸŸ¡ **Phase 20**: Environment awareness (IMU entities done, triggers pending)

### Not Implemented âŒ
- âŒ **Phase 18**: Visual gaze interaction
- âŒ **Phase 21**: Home Assistant scene integration

---

## ðŸ“ˆ Completion Statistics

| Phase | Status | Completion | Notes |
|-------|--------|------------|-------|
| Phase 1-12 | âœ… Complete | 100% | 40 ESPHome entities implemented (Phase 11 LED disabled) |
| Phase 13 | âœ… Complete | 100% | Sendspin audio playback support |
| Phase 14 | âœ… Complete | 95% | Emotion keyword detection with 280+ keywords, 35 categories |
| Phase 15 | âœ… Complete | 100% | Face tracking with DOA, YOLO detection, body follows head |
| Phase 16 | âœ… Complete | 100% | JSON-driven animation system (100Hz control loop) |
| Phase 17 | âœ… Complete | 100% | Antenna sync animation during speech |
| Phase 18 | âŒ Not done | 10% | Camera implemented, missing multi-person gaze |
| Phase 19 | ðŸŸ¡ Partial | 40% | Gravity compensation mode switch, missing teaching flow |
| Phase 20 | ðŸŸ¡ Partial | 30% | IMU sensors exposed, missing trigger logic |
| Phase 21 | âŒ Not done | 0% | Home Assistant scene integration not implemented |
| Phase 22 | âœ… Complete | 100% | Gesture detection with HaGRID ONNX models |
| Phase 24 | âœ… Complete | 100% | System diagnostics with psutil (9 sensors) |
| **v0.9.5** | âœ… Complete | 100% | Modular architecture refactoring |

**Overall Completion**: **Phase 1-17 + 22 + 24 + v0.9.5: ~100%** | **Phase 18-21: ~20%**


---

## ðŸ”§ Daemon Crash Fix (2025-01-05)

### Problem Description
During long-term operation, `reachy_mini daemon` would crash, causing robot to become unresponsive.

### Root Cause
1. **100Hz control loop too frequent** - Calling `robot.set_target()` every 10ms, even when pose hasn't changed
2. **Frequent state queries** - Every entity state read calls `get_status()`, `get_current_head_pose()` etc.
3. **Missing change detection** - Even when pose hasn't changed, continues sending same commands
4. **Zenoh message queue blocking** - Accumulated 150+ messages per second, daemon cannot process in time

### Fix Solution

#### 1. Control loop frequency (movement_manager.py)
```python
# Initially reduced from 100Hz to 20Hz, then later restored to 100Hz
# See "Update (2026-01-12)" below for current status
CONTROL_LOOP_FREQUENCY_HZ = 100  # Now restored to 100Hz
```

#### 2. Add pose change detection (movement_manager.py)
```python
# Only send commands on significant pose changes
if self._last_sent_pose is not None:
    max_diff = max(abs(pose[k] - self._last_sent_pose.get(k, 0.0)) for k in pose.keys())
    if max_diff < 0.001:  # Threshold: 0.001 rad or 0.001 m
        return  # Skip sending
```

#### 3. State query caching (reachy_controller.py)
```python
# Cache daemon status query results
self._cache_ttl = 0.1  # 100ms TTL
self._last_status_query = 0.0

def _get_cached_status(self):
    now = time.time()
    if now - self._last_status_query < self._cache_ttl:
        return self._state_cache.get('status')  # Use cache
    # ... query and update cache
```

#### 4. Head pose query caching (reachy_controller.py)
```python
# Cache get_current_head_pose() and get_current_joint_positions() results
def _get_cached_head_pose(self):
    # Reuse cached results within 100ms
```

### Fix Results

| Metric | Before Fix | After Fix | Improvement |
|--------|------------|-----------|-------------|
| Control message frequency | ~100 msg/s | ~20 msg/s | â†“ 80% |
| State query frequency | ~50 msg/s | ~5 msg/s | â†“ 90% |
| Total Zenoh messages | ~150 msg/s | ~25 msg/s | â†“ 83% |
| Daemon CPU load | Sustained high load | Normal load | Significantly reduced |
| Expected stability | Crash within hours | Stable for days | Major improvement |

### Related Files
- `DAEMON_CRASH_FIX_PLAN.md` - Detailed fix plan and test plan
- `movement_manager.py` - Control loop optimization
- `reachy_controller.py` - State query caching

### Future Optimization Suggestions
1. â³ Dynamic frequency adjustment - 50Hz during motion, 5Hz when idle
2. â³ Batch state queries - Get all states at once
3. â³ Performance monitoring and alerts - Real-time daemon health monitoring

---

## ðŸ”§ Daemon Crash Deep Fix (2026-01-07)

> **Update (2026-01-12)**: After daemon updates and further testing, control loop frequency has been restored to 100Hz (same as `reachy_mini_conversation_app`). The pose change threshold (0.005) and state cache TTL (2s) optimizations remain in place to reduce unnecessary Zenoh messages.

### Problem Description
During long-term operation, `reachy_mini daemon` still crashes, previous fix not thorough enough.

### Root Cause Analysis

Through deep analysis of SDK source code:

1. **Each `set_target()` sends 3 Zenoh messages**
   - `set_target_head_pose()` - 1 message
   - `set_target_antenna_joint_positions()` - 1 message  
   - `set_target_body_yaw()` - 1 message

2. **Daemon control loop is 50Hz**
   - See `reachy_mini/daemon/backend/robot/backend.py`: `control_loop_frequency = 50.0`
   - If message send frequency exceeds 50Hz, daemon may not process in time

3. **Previous 20Hz control loop still too high**
   - 20Hz Ã— 3 messages = 60 messages/second
   - Already exceeds daemon's 50Hz processing capacity

4. **Pose change threshold too small (0.002)**
   - Breathing animation, speech sway, face tracking continuously produce tiny changes
   - Almost every loop triggers `set_target()`

### Fix Solution

#### 1. Control loop frequency history (movement_manager.py)
```python
# Evolution: 100Hz -> 20Hz -> 10Hz -> 100Hz (restored)
# After daemon updates, 100Hz is now stable
CONTROL_LOOP_FREQUENCY_HZ = 100  # Restored to 100Hz (2026-01-12)
```

#### 2. Increase pose change threshold (movement_manager.py)
```python
# Increased from 0.002 to 0.005
# 0.005 rad â‰ˆ 0.29 degrees, still smooth enough
self._pose_change_threshold = 0.005
```

#### 3. Reduce camera/face tracking frequency (camera_server.py)
```python
# Reduced from 15fps to 10fps
fps: int = 10
```

#### 4. Increase state cache TTL (reachy_controller.py)
```python
# Increased from 1 second to 2 seconds
self._cache_ttl = 2.0
```

### Fix Results

> **Note**: Control loop has been restored to 100Hz as of 2026-01-12. The table below shows historical values before restoration.

| Metric | Before (20Hz) | After (10Hz) | Current (100Hz) |
|--------|---------------|--------------|-----------------|
| Control loop frequency | 20 Hz | 10 Hz | 100 Hz (restored) |
| Max Zenoh messages | 60 msg/s | 30 msg/s | ~100 msg/s (optimized) |
| Actual messages (with change detection) | ~40 msg/s | ~15 msg/s | ~30 msg/s |
| Face tracking frequency | 15 Hz | 10 Hz | Adaptive (2-15 Hz) |
| State cache TTL | 1 second | 2 seconds | 2 seconds |
| Expected stability | Crash within hours | Stable operation | Stable (daemon updated) |

### Key Finding

Reference `reachy_mini_conversation_app` uses 100Hz control loop. After daemon updates and optimizations (pose change threshold 0.005, state cache TTL 2s), our app now also runs stably at 100Hz.

### Related Files
- `movement_manager.py` - Control loop frequency and pose threshold
- `camera_server.py` - Face tracking frequency
- `reachy_controller.py` - State cache TTL


---

## ðŸ”§ Microphone Sensitivity Optimization (2026-01-07)

### Problem
Low microphone sensitivity - Need to be very close for voice recognition.

### Solution
Comprehensive ReSpeaker XVF3800 microphone optimization:

| Parameter | Default | Optimized | Notes |
|-----------|---------|-----------|-------|
| AGC | Off | On | Auto volume normalization |
| AGC max gain | ~15dB | 30dB | Better distant speech pickup |
| AGC target level | -25dB | -18dB | Stronger output signal |
| Microphone gain | 1.0x | 2.0x | Base gain doubled |
| Noise suppression | ~0.5 | 0.15 | Reduced speech mis-suppression |

### Result
Microphone sensitivity improved from ~30cm to ~2-3m effective range.

---

## ðŸ”§ v0.5.1 Bug Fixes (2026-01-08)

### Issue 1: Music Not Resuming After Voice Conversation

**Fix**: Sendspin now connects to `music_player` instead of `tts_player`

### Issue 2: Audio Conflict During Voice Assistant Wakeup

**Fix**: Added `pause_sendspin()` and `resume_sendspin()` methods to `audio_player.py`

### Issue 3: Sendspin Sample Rate Optimization

**Fix**: Prioritize 16kHz in Sendspin supported formats (hardware limitation)

---

## ðŸ”§ v0.5.15 Updates (2026-01-11)

### Feature 1: Audio Settings Persistence

AGC Enabled, AGC Max Gain, Noise Suppression settings now persist to `preferences.json`.

### Feature 2: Sendspin Discovery Refactoring

Moved mDNS discovery to `zeroconf.py` for better separation of concerns.


---

### SDK Data Structure Reference

```python
# Motor control mode
class MotorControlMode(str, Enum):
    Enabled = "enabled"              # Torque on, position control
    Disabled = "disabled"            # Torque off
    GravityCompensation = "gravity_compensation"  # Gravity compensation mode

# Daemon state
class DaemonState(Enum):
    NOT_INITIALIZED = "not_initialized"
    STARTING = "starting"
    RUNNING = "running"
    STOPPING = "stopping"
    STOPPED = "stopped"
    ERROR = "error"

# Full state
class FullState:
    control_mode: MotorControlMode
    head_pose: XYZRPYPose  # x, y, z (m), roll, pitch, yaw (rad)
    head_joints: list[float]  # 7 joint angles
    body_yaw: float
    antennas_position: list[float]  # [right, left]
    doa: DoAInfo  # angle (rad), speech_detected (bool)

# IMU data (wireless version only)
imu_data = {
    "accelerometer": [x, y, z],  # m/sÂ²
    "gyroscope": [x, y, z],      # rad/s
    "quaternion": [w, x, y, z],  # Attitude quaternion
    "temperature": float         # Â°C
}

# Safety limits
HEAD_PITCH_ROLL_LIMIT = [-40Â°, +40Â°]
HEAD_YAW_LIMIT = [-180Â°, +180Â°]
BODY_YAW_LIMIT = [-160Â°, +160Â°]
YAW_DELTA_MAX = 65Â°  # Max difference between head and body yaw
```

### ESPHome Protocol Implementation Notes

ESPHome protocol communicates with Home Assistant via protobuf messages. The following message types need to be implemented:

```python
from aioesphomeapi.api_pb2 import (
    # Number entity (volume/angle control)
    ListEntitiesNumberResponse,
    NumberStateResponse,
    NumberCommandRequest,

    # Select entity (motor mode)
    ListEntitiesSelectResponse,
    SelectStateResponse,
    SelectCommandRequest,

    # Button entity (wake/sleep)
    ListEntitiesButtonResponse,
    ButtonCommandRequest,

    # Switch entity (motor switch)
    ListEntitiesSwitchResponse,
    SwitchStateResponse,
    SwitchCommandRequest,

    # Sensor entity (numeric sensors)
    ListEntitiesSensorResponse,
    SensorStateResponse,

    # Binary Sensor entity (boolean sensors)
    ListEntitiesBinarySensorResponse,
    BinarySensorStateResponse,

    # Text Sensor entity (text sensors)
    ListEntitiesTextSensorResponse,
    TextSensorStateResponse,
)
```

## Reference Projects

- [OHF-Voice/linux-voice-assistant](https://github.com/OHF-Voice/linux-voice-assistant)
- [pollen-robotics/reachy_mini](https://github.com/pollen-robotics/reachy_mini)
- [reachy_mini_conversation_app](https://github.com/pollen-robotics/reachy_mini_conversation_app)
- [sendspin-cli](https://github.com/Sendspin/sendspin-cli)
- [home-assistant-voice](https://github.com/esphome/home-assistant-voice-pe/blob/dev/home-assistant-voice.yaml)

---

## ðŸ”§ Code Refactoring & Improvement Plan (v0.9.5)

> Comprehensive improvement plan based on code analysis
> Target Platform: Raspberry Pi CM4 (4GB RAM, 4-core CPU)

### Code Size Statistics (Updated 2026-01-19)

| File | Original | Current | Status |
|------|----------|---------|--------|
| `movement_manager.py` | 1205 | 1173 | âš ï¸ Modularized but still large |
| `voice_assistant.py` | 1097 | 1066 | âœ… Optimized (-3%) |
| `satellite.py` | 1003 | 982 | âœ… Optimized (-2%) |
| `camera_server.py` | 1070 | 966 | âœ… Optimized (-10%) |
| `reachy_controller.py` | 878 | 902 | âš ï¸ Slight increase |
| `entity_registry.py` | 1129 | 735 | âœ… Optimized (-35%) |
| `audio_player.py` | 599 | 624 | âœ… Acceptable |
| `core/service_base.py` | - | 566 | ðŸ†• New module |
| `entities/entity_factory.py` | - | 516 | ðŸ†• New module |

> **Optimization Notes**:
> - `entity_registry.py`: Factory pattern refactoring reduced 394 lines
> - `camera_server.py`: Using `FaceTrackingInterpolator` module reduced 104 lines
> - `satellite.py`: Using `EmotionKeywordDetector` module reduced 21 lines
> - New modular architecture with 5 sub-packages: `core/`, `motion/`, `vision/`, `audio/`, `entities/`

### New Module List (Updated 2026-01-19)

| Directory | Module | Lines | Description |
|-----------|--------|-------|-------------|
| `core/` | `config.py` | 368 | Centralized nested configuration |
| `core/` | `daemon_monitor.py` | 329 | Daemon state monitoring + Sleep detection |
| `core/` | `service_base.py` | 566 | SleepAwareService + RobustOperationMixin |
| `core/` | `sleep_manager.py` | 269 | Sleep/Wake coordination |
| `core/` | `health_monitor.py` | 309 | Service health checking |
| `core/` | `memory_monitor.py` | 275 | Memory usage monitoring |
| `core/` | `exceptions.py` | 71 | Custom exception classes |
| `motion/` | `antenna.py` | 197 | Antenna freeze/unfreeze control |
| `motion/` | `pose_composer.py` | 273 | Pose composition utilities |
| `motion/` | `gesture_actions.py` | 383 | Gesture to action mapping |
| `motion/` | `state_machine.py` | 91 | State machine definitions |
| `motion/` | `smoothing.py` | 198 | Smoothing/transition algorithms |
| `vision/` | `frame_processor.py` | 268 | Adaptive frame rate management |
| `vision/` | `face_tracking_interpolator.py` | 225 | Face lost interpolation |
| `audio/` | `doa_tracker.py` | 206 | Direction of Arrival tracking |
| `audio/` | `microphone.py` | 230 | ReSpeaker microphone optimization |
| `entities/` | `entity_factory.py` | 516 | Entity factory pattern |
| `entities/` | `entity_keys.py` | 155 | Entity key constants |
| `entities/` | `event_emotion_mapper.py` | 341 | HA event to emotion mapping |
| `entities/` | `emotion_detector.py` | 119 | LLM emotion keyword detection |

### Improvement Plan Status

#### Phase 1: Sleep State Management âœ… Complete

- [x] Create `core/daemon_monitor.py` - DaemonStateMonitor
- [x] Create `core/service_base.py` - SleepAwareService interface
- [x] Create `core/sleep_manager.py` - SleepManager
- [x] All services implement `suspend()`/`resume()` methods
- [x] Add Sleep state sensor to HA
- [ ] Test complete Sleep/Wake cycle

#### Phase 2: Code Modularization âœ… Complete

- [x] Create new directory structure (`core/`, `motion/`, `audio/`, `vision/`, `entities/`)
- [x] Extract from `movement_manager.py` â†’ `motion/antenna.py`, `motion/pose_composer.py`
- [x] Extract from `camera_server.py` â†’ `vision/frame_processor.py`, `vision/face_tracking_interpolator.py`
- [x] Extract from `entity_registry.py` â†’ `entities/entity_factory.py`, `entities/entity_keys.py`
- [x] Create `core/config.py` for centralized configuration
- [x] Ensure no circular dependencies

#### Phase 3: Stability & Performance âœ… Complete

- [x] Create `core/exceptions.py` - Custom exception classes
- [x] Implement `RobustOperationMixin` - Unified error handling
- [x] `CameraServer` implements Context Manager pattern
- [x] Improve `CameraServer` resource cleanup
- [x] Fix MJPEG client tracking (proper register/unregister)
- [x] Add `core/health_monitor.py` - Service health checking
- [x] Add `core/memory_monitor.py` - Memory usage monitoring
- [ ] Long-running stability test (24h+)

#### Phase 4: Feature Enhancements âœ… Complete

- [x] Create `motion/gesture_actions.py` - GestureActionMapper
- [x] Add `animations/gesture_mappings.json` - Gesture action config
- [x] Create `audio/doa_tracker.py` - DOATracker
- [x] Implement sound source tracking with motion control integration
- [x] Create `entities/event_emotion_mapper.py` - EventEmotionMapper
- [x] Add `animations/event_mappings.json` - HA event emotion mapping
- [x] Add DOA tracking toggle HA entity

### SDK Compatibility Verification âœ… Passed

| API Call | Status | Notes |
|----------|--------|-------|
| `set_target(head, antennas, body_yaw)` | âœ… | Correct usage |
| `goto_target()` | âœ… | Correct usage |
| `look_at_image(u: int, v: int)` | âœ… | Fixed floatâ†’int |
| `create_head_pose(degrees=False)` | âœ… | Using radians |
| `compose_world_offset()` | âœ… | SDK function correctly called |
| `linear_pose_interpolation()` | âœ… | Has fallback implementation |
| Body yaw range | âœ… | Clamped to Â±160Â° |

---

## ðŸ”§ v0.9.5 Updates (2026-01-19)

### Major Changes: Modular Architecture Refactoring

The codebase has been restructured into a modular architecture with 5 sub-packages:

| Package | Purpose | Key Modules |
|---------|---------|-------------|
| `core/` | Core infrastructure | `config.py`, `service_base.py`, `sleep_manager.py`, `health_monitor.py` |
| `motion/` | Motion control | `antenna.py`, `pose_composer.py`, `gesture_actions.py`, `smoothing.py` |
| `vision/` | Vision processing | `frame_processor.py`, `face_tracking_interpolator.py` |
| `audio/` | Audio processing | `microphone.py`, `doa_tracker.py` |
| `entities/` | HA entity management | `entity_factory.py`, `entity_keys.py`, `event_emotion_mapper.py` |

### New Features

1. **Direct Sleep/Wake Callbacks**
   - HA sleep/wake buttons directly call `suspend()`/`resume()` on services
   - More reliable than polling-based approach

2. **Synchronous Camera Resume**
   - `camera_server.resume_from_suspend()` is now synchronous
   - Ensures camera is ready before voice assistant starts listening

### Audio Optimizations

| Parameter | Before | After | Improvement |
|-----------|--------|-------|-------------|
| Audio chunk size | 1024 samples | 256 samples | 64ms â†’ 16ms latency |
| Audio loop delay | 10ms | 1ms | Faster VAD response |
| Stereoâ†’Mono | Mean of channels | First channel | Cleaner signal |

### Code Quality Improvements

- Removed all legacy/compatibility code
- Centralized configuration in nested dataclasses
- NaN/Inf cleaning in audio pipeline
- Rotation clamping in face tracking to prevent IK collisions